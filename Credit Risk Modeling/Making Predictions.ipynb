{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing cleaned and prepared data from the lats script, Preparing the Features.  \n",
    "As we prepared the data, we removed columns that had data leakage issues, contained redundant information, or required additional processing to turn into useful features. We cleaned features that had formatting issues, and converted categorical columns to dummy variables.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 38649 entries, 0 to 38648\n",
      "Data columns (total 40 columns):\n",
      "Unnamed: 0                             38649 non-null int64\n",
      "Unnamed: 0.1                           38649 non-null int64\n",
      "loan_amnt                              38649 non-null float64\n",
      "int_rate                               38649 non-null float64\n",
      "installment                            38649 non-null float64\n",
      "emp_length                             38649 non-null int64\n",
      "annual_inc                             38649 non-null float64\n",
      "loan_status                            38649 non-null int64\n",
      "dti                                    38649 non-null float64\n",
      "delinq_2yrs                            38649 non-null float64\n",
      "inq_last_6mths                         38649 non-null float64\n",
      "open_acc                               38649 non-null float64\n",
      "pub_rec                                38649 non-null float64\n",
      "revol_bal                              38649 non-null float64\n",
      "revol_util                             38649 non-null float64\n",
      "total_acc                              38649 non-null float64\n",
      "home_ownership_MORTGAGE                38649 non-null int64\n",
      "home_ownership_NONE                    38649 non-null int64\n",
      "home_ownership_OTHER                   38649 non-null int64\n",
      "home_ownership_OWN                     38649 non-null int64\n",
      "home_ownership_RENT                    38649 non-null int64\n",
      "verification_status_Not Verified       38649 non-null int64\n",
      "verification_status_Source Verified    38649 non-null int64\n",
      "verification_status_Verified           38649 non-null int64\n",
      "purpose_car                            38649 non-null int64\n",
      "purpose_credit_card                    38649 non-null int64\n",
      "purpose_debt_consolidation             38649 non-null int64\n",
      "purpose_educational                    38649 non-null int64\n",
      "purpose_home_improvement               38649 non-null int64\n",
      "purpose_house                          38649 non-null int64\n",
      "purpose_major_purchase                 38649 non-null int64\n",
      "purpose_medical                        38649 non-null int64\n",
      "purpose_moving                         38649 non-null int64\n",
      "purpose_other                          38649 non-null int64\n",
      "purpose_renewable_energy               38649 non-null int64\n",
      "purpose_small_business                 38649 non-null int64\n",
      "purpose_vacation                       38649 non-null int64\n",
      "purpose_wedding                        38649 non-null int64\n",
      "term_ 36 months                        38649 non-null int64\n",
      "term_ 60 months                        38649 non-null int64\n",
      "dtypes: float64(12), int64(28)\n",
      "memory usage: 11.8 MB\n"
     ]
    }
   ],
   "source": [
    "loans= pd.read_csv(\"data/cleaned_loans_2007.csv\")\n",
    "loans.info()\n",
    "# loan_status =0, means that loan not paid and 1 means otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are a conservative investor, who is only interested in borrowers paying back on time. So, our main objective is to make money, we want to fund enough loans that are paid off on time to offset our losses from loans that aren't paid off. Our error metric will help us determine if our algorithm will make us money or lose us money.  \n",
    "In this case, we're primarily concerned with false positives and false negatives. We would want to minimize risk, and avoid false positives as much as possible. We'd be more okay with missing out on opportunities (false negatives) than they would be with funding a risky loan (false positives)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that there was an imbalance in classes which would affect the model's result badly so to overcome this, we should use the following ways: \n",
    "- Use oversampling and undersampling to ensure that the classifier gets input that has a balanced number of each class.\n",
    "- Tell the classifier to penalize misclassifications of the less prevalent class more than the other class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first option is quite difficult to achieve so we're going to go with the second option as it is easier to implement with scikit-learn as well.  \n",
    "\n",
    "We can do this by setting the class_weight parameter to balanced when creating the LogisticRegression instance. This tells scikit-learn to penalize the misclassification of the minority class during the training process. The penalty means that the logistic regression classifier pays more attention to correctly classifying rows where loan_status is 0. This lowers accuracy when loan_status is 1, but raises accuracy when loan_status is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44311266826479806 0.6878950219707458\n"
     ]
    }
   ],
   "source": [
    "cols = loans.columns\n",
    "train_cols = cols.drop(\"loan_status\")\n",
    "features = loans[train_cols]\n",
    "target = loans[\"loan_status\"]\n",
    "\n",
    "lr = LogisticRegression(class_weight=\"balanced\")\n",
    "predictions= cross_val_predict(lr, features, target, cv=3)\n",
    "\n",
    "predictions= pd.Series(predictions)\n",
    "\n",
    "tn_filter = (predictions == 0) & (loans[\"loan_status\"] == 0)\n",
    "tn = len(predictions[tn_filter])\n",
    "\n",
    "tp_filter = (predictions == 1) & (loans[\"loan_status\"] == 1)\n",
    "tp = len(predictions[tp_filter])\n",
    "\n",
    "fn_filter = (predictions == 0) & (loans[\"loan_status\"] == 1)\n",
    "fn = len(predictions[fn_filter])\n",
    "\n",
    "fp_filter = (predictions == 1) & (loans[\"loan_status\"] == 0)\n",
    "fp = len(predictions[fp_filter])\n",
    "\n",
    "fpr= fp/(fp+tn)\n",
    "tpr= tp/(tp+fn)\n",
    "\n",
    "print(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can see the TPR is higher than the FPR, which is good and what we want. But this also means that as a conservative investor we'd decide to fund 69% of the total loans, rejecting a good amount of loans.  \n",
    "To improve the TPR further we could **_manually adjust the penalty_** to make it harsher, the scikit learn puts a penalty value of the ratio of the number of 1s/ number of 0s when set to balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20302415637101234 0.3507494131102149\n"
     ]
    }
   ],
   "source": [
    "penalty = {\n",
    "    0: 10,\n",
    "    1: 1\n",
    "}\n",
    "\n",
    "lr = LogisticRegression(class_weight=penalty)\n",
    "predictions= cross_val_predict(lr, features, target, cv=3)\n",
    "\n",
    "predictions= pd.Series(predictions)\n",
    "\n",
    "tn_filter = (predictions == 0) & (loans[\"loan_status\"] == 0)\n",
    "tn = len(predictions[tn_filter])\n",
    "\n",
    "tp_filter = (predictions == 1) & (loans[\"loan_status\"] == 1)\n",
    "tp = len(predictions[tp_filter])\n",
    "\n",
    "fn_filter = (predictions == 0) & (loans[\"loan_status\"] == 1)\n",
    "fn = len(predictions[fn_filter])\n",
    "\n",
    "fp_filter = (predictions == 1) & (loans[\"loan_status\"] == 0)\n",
    "fp = len(predictions[fp_filter])\n",
    "\n",
    "fpr= fp/(fp+tn)\n",
    "tpr= tp/(tp+fn)\n",
    "\n",
    "print(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like introducing the manual penalty decreased the FPR to 20% from 45%.  \n",
    "There is always more scope to play around with penalty values and get better accuracy but we can try a different model now, like a Random Forest.  \n",
    "The Logistic Regression models are only able to work with linear data but Random Forests can work better by working with all those features that are non-linearly related to the target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6123916651300019 0.6205080358755192\n"
     ]
    }
   ],
   "source": [
    "rfc= RandomForestClassifier(random_state=1, class_weight=\"balanced\") \n",
    "# random_state is set to 1 so that the predictions don't vary due to random chance\n",
    "predictions= cross_val_predict(rfc, features, target, cv=3)\n",
    "\n",
    "predictions= pd.Series(predictions)\n",
    "\n",
    "tn_filter = (predictions == 0) & (loans[\"loan_status\"] == 0)\n",
    "tn = len(predictions[tn_filter])\n",
    "\n",
    "tp_filter = (predictions == 1) & (loans[\"loan_status\"] == 1)\n",
    "tp = len(predictions[tp_filter])\n",
    "\n",
    "fn_filter = (predictions == 0) & (loans[\"loan_status\"] == 1)\n",
    "fn = len(predictions[fn_filter])\n",
    "\n",
    "fp_filter = (predictions == 1) & (loans[\"loan_status\"] == 0)\n",
    "fp = len(predictions[fp_filter])\n",
    "\n",
    "fpr= fp/(fp+tn)\n",
    "tpr= tp/(tp+fn)\n",
    "\n",
    "print(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Introducing harsher penalty_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6369168356997972 0.6471738999578643\n"
     ]
    }
   ],
   "source": [
    "penalty = {\n",
    "    0: 10,\n",
    "    1: 1\n",
    "}\n",
    "\n",
    "rfc= RandomForestClassifier(random_state=1, class_weight=penalty) \n",
    "# random_state is set to 1 so that the predictions don't vary due to random chance\n",
    "predictions= cross_val_predict(rfc, features, target, cv=3)\n",
    "\n",
    "predictions= pd.Series(predictions)\n",
    "\n",
    "tn_filter = (predictions == 0) & (loans[\"loan_status\"] == 0)\n",
    "tn = len(predictions[tn_filter])\n",
    "\n",
    "tp_filter = (predictions == 1) & (loans[\"loan_status\"] == 1)\n",
    "tp = len(predictions[tp_filter])\n",
    "\n",
    "fn_filter = (predictions == 0) & (loans[\"loan_status\"] == 1)\n",
    "fn = len(predictions[fn_filter])\n",
    "\n",
    "fp_filter = (predictions == 1) & (loans[\"loan_status\"] == 0)\n",
    "fp = len(predictions[fp_filter])\n",
    "\n",
    "fpr= fp/(fp+tn)\n",
    "tpr= tp/(tp+fn)\n",
    "\n",
    "print(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Unfortunately, using a random forest classifier (even with harsher penalties) didn't improve our false positive rate. The model is likely weighting too heavily on the 1 class, and still mostly predicting 1s.  \n",
    "Our best model was the Logistic Regression with 20% FPR. For a conservative investor, this means that they make money as long as the interest rate is high enough to offset the losses from 7% of borrowers defaulting, and that the pool of 20% of borrowers is large enough to make enough interest money to offset the losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Scope\n",
    "- We can tweak the penalties further.\n",
    "- We can try models other than a random forest and logistic regression.\n",
    "- We can use some of the columns we discarded to generate better features.\n",
    "- We can ensemble multiple models to get more accurate predictions.\n",
    "- We can tune the parameters of the algorithm to achieve higher performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
